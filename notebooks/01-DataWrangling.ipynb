{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a526be2d-e18a-4391-ad18-9f0d1f5777b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from constants import EVALUATION_DATE_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c13f4-576e-48c2-a470-aeda7a04c3c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Wrangling\n",
    "\n",
    "1. Unify the file formats (i.e., `json` --> `csv`)\n",
    "1. Merge metadata from multiple sources (i.e., NIH Common Fund repositories and Database Commons)\n",
    "1. Add URLs of resources if missing (i.e., Journal homepages using `Sourceid`)\n",
    "1. Add manually collected subpages (e.g., docs, search page, etc)\n",
    "1. Add our own IDs for individual resources by updating (`global_data-portal_id_map.csv`)\n",
    "<!-- 1. Add resource connection status -->\n",
    "\n",
    "**To-Do**\n",
    "- [ ] Merge NIH data portals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Portals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the original data\n",
    "\"\"\"\n",
    "df = pd.read_json(os.path.join('../input', EVALUATION_DATE_FOLDER, 'database-commons.json'))\n",
    "\n",
    "# df = df.head(10) # for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We use underscore lowercase column names\n",
    "\"\"\"\n",
    "df.columns = (df.columns.str.replace('(?<=[a-z])(?=[A-Z])', '_', regex=True).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Drop columns that we don't need\n",
    "\"\"\"\n",
    "df.drop(columns=['biodb_ranks', 'rating_list'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The id from the sources are consistently \"source_id\"\n",
    "The values should be a string type, and it has the prefix that represents the source (e.g. dc_ for Database Commons)\n",
    "\"\"\"\n",
    "df.rename(columns={ \"db_id\": \"source_id\" }, inplace=True)\n",
    "df.source_id = df.source_id.apply(lambda x: 'dc_' + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some columns from data commons are in json format, we need to convert them to string\n",
    "Example: [{ \"id\": 1, \"name\": \"foo\" }, { \"id\": 2, \"name\": \"bar\" }] --> 'foo, bar'\n",
    "\"\"\"\n",
    "json_column_names_and_keys = {\n",
    "    'data_type_list': 'datatypeName', \n",
    "    'category_list': 'name',\n",
    "    'keywords_list': 'name',\n",
    "    'data_object_list': 'name',\n",
    "    'organism_list': 'organismName',\n",
    "    'theme_list': 'name'\n",
    "}\n",
    "\n",
    "for (column, key) in json_column_names_and_keys.items():\n",
    "    df[column] = df[column].apply(lambda x: ', '.join([object[key] for object in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes too much time. We will instead just run WAVE API requests for all URLs.\n",
    "# # Check the connection status and put that as a `reachable` column\n",
    "\n",
    "# # TODO: a faster way to do this?\n",
    "# def check_connection_status(url):\n",
    "#     print(url)\n",
    "#     try:\n",
    "#         status = requests.get(url)\n",
    "#     except Exception:\n",
    "#         return False\n",
    "#     return status.status_code == 200\n",
    "    \n",
    "# df['reachable'] = df['url'].apply(lambda x: check_connection_status(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the ID Mapping Table (`data-portal_id_map.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create or update a mapping table (i.e., `data-portal_id_map.csv`)\n",
    "\"\"\"\n",
    "# Load the data first\n",
    "file = Path(os.path.join('../output', 'data-portal_id_map.csv'))\n",
    "if file.exists():\n",
    "    df_map = pd.read_csv(file)\n",
    "else:\n",
    "    df_map = pd.DataFrame(columns=['id', 'source_id', 'date_added'])\n",
    "\n",
    "# Find rows that does not already exist in the table\n",
    "df_temp = df_map.merge(df, how='outer', on='source_id', indicator=True)\n",
    "df_new_resources = df_temp[df_temp['_merge'] == 'right_only'][['source_id']]\n",
    "\n",
    "# Ensure to assign new `id`s, i.e., New `id` == max id + 1\n",
    "max_id = df_map.id.max()\n",
    "max_id = 0 if max_id is np.nan else max_id\n",
    "new_id = max_id + 1\n",
    "\n",
    "df_new_resources.insert(0, 'id', range(new_id, new_id + len(df_new_resources)))\n",
    "df_new_resources['date_added'] = pd.to_datetime('today').strftime('%m-%d-%Y')\n",
    "\n",
    "pd.concat([df_map, df_new_resources], axis=0).to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Metadata (`data-portal_metadata.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add the `id` column to the original metadata\n",
    "\"\"\"\n",
    "df_map = pd.read_csv(file)\n",
    "df_meta = df_map[['id', 'source_id']].merge(df, how='right', on='source_id')\n",
    "df_meta.to_csv(os.path.join('../output', EVALUATION_DATE_FOLDER, 'data-portal_metadata.csv'), index=False)\n",
    "df_meta.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Homepages (`data-portal_pages.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create `data-portal_pages.csv` and add homepage urls.\n",
    "\"\"\"\n",
    "df_meta = pd.read_csv(os.path.join('../output', EVALUATION_DATE_FOLDER, 'data-portal_metadata.csv'))\n",
    "\n",
    "df_pages = df_meta[['id', 'url']].copy()\n",
    "\n",
    "df_pages['page_type'] = 'home'\n",
    "df_pages['page_id'] = df_pages['id']\n",
    "df_pages['page_id'] = df_pages['page_id'].apply(lambda x: str(x) + '_' + 'home')\n",
    "\n",
    "df_pages.to_csv(os.path.join('../output', EVALUATION_DATE_FOLDER, 'data-portal_pages.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Subpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add manually collected subpages to `data-portal_pages.csv` from `URL Collection for Subpages - Data Portals.csv`\n",
    "\"\"\"\n",
    "df_subpages = pd.read_csv(os.path.join('../input', EVALUATION_DATE_FOLDER, 'URL Collection for Subpages - Data Portals.csv'))\n",
    "\n",
    "# add prefix to the source_id, following our naming convention\n",
    "df_subpages.source_id = df_subpages.source_id.apply(lambda x: 'dc_' + str(x))\n",
    "\n",
    "# Replace the descriptive page_type with the code\n",
    "page_type_map = {\n",
    "    'Home': 'home',\n",
    "    'Search/Filter': 'search',\n",
    "    'Search Result': 'search_result',\n",
    "    'Data Entity (Detail Page)': 'data_entity',\n",
    "    'Documentation or FAQ': 'documentation',\n",
    "}\n",
    "df_subpages.page_type = df_subpages.page_type.apply(lambda x: page_type_map[x])\n",
    "\n",
    "# Append our `id`\n",
    "df_map = pd.read_csv(os.path.join('../output', 'data-portal_id_map.csv'))\n",
    "df_subpages = df_subpages.merge(df_map[['id', 'source_id']], how='left', on='source_id')\n",
    "\n",
    "# Add `page_id` using both `id` and `page_type`\n",
    "df_subpages['page_id'] = df_subpages['id'].astype(str) + '_' + df_subpages['page_type']\n",
    "\n",
    "# Load the existing pages\n",
    "path_pages = os.path.join('../output', EVALUATION_DATE_FOLDER, 'data-portal_pages.csv')\n",
    "df_pages = pd.read_csv(path_pages)\n",
    "\n",
    "# Append the new pags\n",
    "df_pages = df_pages.merge(df_subpages[['id', 'url', 'page_type', 'page_id']], how='outer', on=['page_id', 'page_type', 'id'])\n",
    "\n",
    "# Now that we have two versions of URLs, we prefer to keep the manually collected ones (`url_y`) over the orignal ones (`url_x`) if exist\n",
    "df_pages['url'] = df_pages.url_y.combine_first(df_pages.url_x)\n",
    "\n",
    "# drop the temporary columns\n",
    "df_pages.drop(columns=['url_x', 'url_y'], inplace=True)\n",
    "\n",
    "# save the file\n",
    "df_pages.to_csv(path_pages, index=False)\n",
    "\n",
    "df_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the original data\n",
    "\"\"\"\n",
    "df = pd.read_csv(os.path.join('../input', EVALUATION_DATE_FOLDER, 'scimagojr 2022.csv'), sep=';')\n",
    "\n",
    "# df = df.head(5) # for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using `Sourceid` of SJR, get URLs of individual journal portals\n",
    "TODO: Reuse the previously identified home pages\n",
    "\"\"\"\n",
    "def infer_homepage(Sourceid):\n",
    "    info_url = f'https://www.scimagojr.com/journalsearch.php?q={Sourceid}&tip=sid&clean=0'\n",
    "    html_text = requests.get(info_url).text\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    urls = soup.find_all('a', text=re.compile('Homepage'))\n",
    "    if len(urls) > 0:\n",
    "        return urls[0].get('href')\n",
    "    else:\n",
    "        print(f'No homepage found for {Sourceid}')\n",
    "        return None\n",
    "\n",
    "df['url'] = df['Sourceid'].apply(lambda x: infer_homepage(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We use underscore lowercase column names\n",
    "\"\"\"\n",
    "df.columns = (df.columns.str.replace('(?<=[a-z])(?=[A-Z])', '_', regex=True).str.lower())\n",
    "df.columns = (df.columns.str.replace('.', '')) # remove dots\n",
    "df.columns = (df.columns.str.replace('(', '')) # remove parentheses\n",
    "df.columns = (df.columns.str.replace(')', ''))\n",
    "df.columns = (df.columns.str.replace('/', 'per')) # replace slash with \"per\"\n",
    "df.columns = (df.columns.str.replace(' ', '_')) # replace space with underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The id from the sources are consistently \"source_id\"\n",
    "The values should be a string type, and it has the prefix that represents the source (e.g. dc_ for Database Commons)\n",
    "\"\"\"\n",
    "df.rename(columns={ \"sourceid\": \"source_id\" }, inplace=True)\n",
    "df.source_id = df.source_id.apply(lambda x: 'sjr_' + str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the ID Mapping Table (`journal-portal_id_map.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create or update a mapping table (i.e., `data-portal_id_map.csv`)\n",
    "\"\"\"\n",
    "# Load the data first\n",
    "file = Path(os.path.join('../output', 'journal-portal_id_map.csv'))\n",
    "if file.exists():\n",
    "    df_map = pd.read_csv(file)\n",
    "else:\n",
    "    df_map = pd.DataFrame(columns=['id', 'source_id', 'date_added'])\n",
    "\n",
    "# Find rows that does not already exist in the table\n",
    "df_temp = df_map.merge(df, how='outer', on='source_id', indicator=True)\n",
    "df_new_resources = df_temp[df_temp['_merge'] == 'right_only'][['source_id']]\n",
    "\n",
    "# Ensure to assign new `id`s, i.e., New `id` == max id + 1\n",
    "max_id = df_map.id.max()\n",
    "max_id = 0 if max_id is np.nan else max_id\n",
    "new_id = max_id + 1\n",
    "\n",
    "df_new_resources.insert(0, 'id', range(new_id, new_id + len(df_new_resources)))\n",
    "df_new_resources['date_added'] = pd.to_datetime('today').strftime('%m-%d-%Y')\n",
    "\n",
    "pd.concat([df_map, df_new_resources], axis=0).to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Metadata of Journal Portals (`journal-portal_metadata.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add the `id` column to the original metadata\n",
    "\"\"\"\n",
    "df_map = pd.read_csv(file)\n",
    "df_meta = df_map[['id', 'source_id']].merge(df, how='right', on='source_id')\n",
    "df_meta.to_csv(os.path.join('../output', EVALUATION_DATE_FOLDER, 'journal-portal_metadata.csv'), index=False)\n",
    "df_meta.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Pages of Journal Portals (`journal-portal_pages.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create `*_pages.csv` and add homepage urls.\n",
    "\"\"\"\n",
    "df_meta = pd.read_csv(os.path.join('../output', EVALUATION_DATE_FOLDER, 'journal-portal_metadata.csv'))\n",
    "\n",
    "df_pages = df_meta[['id', 'url']].copy()\n",
    "\n",
    "df_pages['page_type'] = 'home'\n",
    "df_pages['page_id'] = df_pages['id']\n",
    "df_pages['page_id'] = df_pages['page_id'].apply(lambda x: str(x) + '_' + 'home')\n",
    "\n",
    "df_pages.to_csv(os.path.join('../output', EVALUATION_DATE_FOLDER, 'journal-portal_pages.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Subpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add manually collected subpages to `journal-portal_pages.csv` from `URL Collection for Subpages - Journals.csv`\n",
    "\"\"\"\n",
    "df_subpages = pd.read_csv(os.path.join('../input', EVALUATION_DATE_FOLDER, 'URL Collection for Subpages - Journals.csv'))\n",
    "\n",
    "# add prefix to the source_id, following our naming convention\n",
    "df_subpages.source_id = df_subpages.source_id.apply(lambda x: 'sjr_' + str(x))\n",
    "\n",
    "# Replace the descriptive page_type with the code\n",
    "page_type_map = {\n",
    "    'Home': 'home',\n",
    "    'Latest Open Access Research Article': 'research_article',\n",
    "    'Latest None-research Article': 'none_research_article',\n",
    "    'Article Search Result': 'article_search_result',\n",
    "    'Latest Issue': 'latest_issue',\n",
    "}\n",
    "df_subpages.page_type = df_subpages.page_type.apply(lambda x: page_type_map[x])\n",
    "\n",
    "# Append our `id`\n",
    "df_map = pd.read_csv(os.path.join('../output', 'journal-portal_id_map.csv'))\n",
    "df_subpages = df_subpages.merge(df_map[['id', 'source_id']], how='left', on='source_id')\n",
    "\n",
    "# Add `page_id` using both `id` and `page_type`\n",
    "df_subpages['page_id'] = df_subpages['id'].astype(str) + '_' + df_subpages['page_type']\n",
    "\n",
    "# Load the existing pages\n",
    "path_pages = os.path.join('../output', EVALUATION_DATE_FOLDER, 'journal-portal_pages.csv')\n",
    "df_pages = pd.read_csv(path_pages)\n",
    "\n",
    "# Append the new pags\n",
    "df_pages = df_pages.merge(df_subpages[['id', 'url', 'page_type', 'page_id']], how='outer', on=['page_id', 'page_type', 'id'])\n",
    "\n",
    "# Now that we have two versions of URLs, we prefer to keep the manually collected ones (`url_y`) over the orignal ones (`url_x`) if exist\n",
    "df_pages['url'] = df_pages.url_y.combine_first(df_pages.url_x)\n",
    "\n",
    "# drop the temporary columns\n",
    "df_pages.drop(columns=['url_x', 'url_y'], inplace=True)\n",
    "\n",
    "# save the file\n",
    "df_pages.to_csv(path_pages, index=False)\n",
    "\n",
    "df_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
